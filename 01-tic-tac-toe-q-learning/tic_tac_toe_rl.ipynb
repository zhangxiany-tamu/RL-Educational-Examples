{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Tic-Tac-Toe\n",
    "\n",
    "This notebook demonstrates key reinforcement learning concepts using the classic game of tic-tac-toe as an example.\n",
    "\n",
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties for its actions and learns to maximize cumulative rewards over time.\n",
    "\n",
    "### Key Components:\n",
    "- **Agent**: The learner/decision maker\n",
    "- **Environment**: The world the agent interacts with\n",
    "- **State**: Current situation of the agent\n",
    "- **Action**: What the agent can do\n",
    "- **Reward**: Feedback from the environment\n",
    "- **Policy**: Strategy that maps states to actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-Tac-Toe Environment\n",
    "\n",
    "Let's create a simple tic-tac-toe environment where our RL agent can learn to play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1  # 1 for X, -1 for O\n",
    "        \n",
    "    def reset(self, random_start=False):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        if random_start:\n",
    "            self.current_player = random.choice([1, -1])\n",
    "        else:\n",
    "            self.current_player = 1\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Convert board to a string representation for use as dictionary key\n",
    "        return str(self.board.flatten())\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        # Return list of valid positions (empty squares)\n",
    "        valid_actions = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i, j] == 0:\n",
    "                    valid_actions.append((i, j))\n",
    "        return valid_actions\n",
    "    \n",
    "    def make_move(self, action):\n",
    "        row, col = action\n",
    "        if self.board[row, col] != 0:\n",
    "            return False  # Invalid move\n",
    "        \n",
    "        self.board[row, col] = self.current_player\n",
    "        return True\n",
    "    \n",
    "    def check_winner(self):\n",
    "        # Check rows, columns, and diagonals\n",
    "        for i in range(3):\n",
    "            if abs(sum(self.board[i, :])) == 3:\n",
    "                return self.board[i, 0]\n",
    "            if abs(sum(self.board[:, i])) == 3:\n",
    "                return self.board[0, i]\n",
    "        \n",
    "        # Check diagonals\n",
    "        if abs(sum([self.board[i, i] for i in range(3)])) == 3:\n",
    "            return self.board[0, 0]\n",
    "        if abs(sum([self.board[i, 2-i] for i in range(3)])) == 3:\n",
    "            return self.board[0, 2]\n",
    "        \n",
    "        return 0  # No winner\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        return self.check_winner() != 0 or len(self.get_valid_actions()) == 0\n",
    "    \n",
    "    def get_reward(self, player):\n",
    "        winner = self.check_winner()\n",
    "        if winner == player:\n",
    "            return 1  # Win\n",
    "        elif winner == -player:\n",
    "            return -1  # Loss\n",
    "        else:\n",
    "            return 0  # Draw or game not over\n",
    "    \n",
    "    def display(self):\n",
    "        symbols = {0: ' ', 1: 'X', -1: 'O'}\n",
    "        print(\"\\n   0   1   2\")\n",
    "        for i in range(3):\n",
    "            print(f\"{i}  {symbols[self.board[i,0]]} | {symbols[self.board[i,1]]} | {symbols[self.board[i,2]]}\")\n",
    "            if i < 2:\n",
    "                print(\"  -----------\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Q-Learning Agent\n\nQ-Learning is a model-free reinforcement learning algorithm that learns the quality of actions, telling an agent what action to take under what circumstances.\n\n### Mathematical Foundation\n\n**Bellman Equation for Q-Values:**\n\nThe optimal Q-function satisfies the Bellman optimality equation:\n```\nQ*(s,a) = E[r + γ·max Q*(s',a') | s,a]\n```\n\nThis states that the optimal Q-value for state-action pair (s,a) equals the expected immediate reward plus the discounted maximum Q-value of the next state.\n\n**From Bellman to Q-Learning:**\n\n1. **Bellman Equation** (theoretical optimum):\n   ```\n   Q*(s,a) = E[r + γ·max Q*(s',a')]\n   ```\n\n2. **Sample-based approximation** (replace expectation with sample):\n   ```\n   Q(s,a) ≈ r + γ·max Q(s',a')\n   ```\n\n3. **Iterative update** (gradually adjust towards target):\n   ```\n   Q(s,a) ← Q(s,a) + α[Target - Current]\n   Q(s,a) ← Q(s,a) + α[r + γ·max Q(s',a') - Q(s,a)]\n   ```\n\n**Q-Learning Update Rule:**\n```\nQ(s,a) ← Q(s,a) + α[r + γ·max Q(s',a') - Q(s,a)]\n```\n\nWhere:\n- **Q(s,a)**: Quality value for taking action `a` in state `s`\n- **α (alpha)**: Learning rate ∈ [0,1] - controls how much new information overrides old\n- **γ (gamma)**: Discount factor ∈ [0,1] - importance of future rewards\n- **r**: Immediate reward received\n- **s'**: Next state after taking action `a`\n- **max Q(s',a')**: Maximum Q-value among all possible actions in next state\n\n**Temporal Difference Error:**\n```\nδ = r + γ·max Q(s',a') - Q(s,a)\n```\n\nThis error represents the difference between the Bellman target and current estimate, driving convergence to the optimal Q-function.\n\n### Q-Table Structure and State Representation\n\n**What is the Q-Table?**\nThe Q-table is a lookup table that stores quality values for state-action pairs:\n```python\nQ[state][action] = expected_reward\n```\n\n**State Representation:**\n- **State**: Current board configuration only (not history)\n- **Format**: String representation of flattened 3x3 board\n- **Example**: `\"[0 0 0 0 1 0 0 0 0]\"` (X in center, rest empty)\n- **Values**: 0=empty, 1=X, -1=O\n\n**Important Properties:**\n- **Markovian**: Only current board state matters for optimal play\n- **Sparse**: Q-table only contains states encountered during training (~3000 out of 19,683 possible)\n- **Valid Only**: No invalid states (like too many X's) are stored\n- **Dynamic Growth**: Q-table starts empty and grows during training\n\n**Q-Values vs Probabilities:**\n- **Q-values**: Expected future reward (can be negative, positive, or zero)\n- **NOT probabilities**: Q-values represent quality, not likelihood\n- **Action selection**: Uses epsilon-greedy policy to convert Q-values to actions\n\n### Epsilon-Greedy Policy and Learning Evolution\n\n**Epsilon-Greedy Strategy:**\n```\nπ(s) = {\n  random action with probability ε (exploration)\n  argmax Q(s,a) with probability 1-ε (exploitation)\n}\n```\n\n**Epsilon Decay During Training:**\n- **Initial**: ε = 0.1 (10% random exploration)\n- **Decay**: ε = ε × 0.995 after each episode\n- **Minimum**: ε = 0.01 (1% random, never goes to zero)\n- **Purpose**: Balance exploration (learning) vs exploitation (using knowledge)\n\n**Training vs Playing Behavior:**\n- **During Training**: Uses epsilon-greedy (some randomness for learning)\n- **When Playing**: Sets ε = 0 (pure greedy, no randomness, optimal play)\n\n### Key Concepts:\n- **Q-Table**: Stores quality values for state-action pairs\n- **Exploration vs Exploitation**: Balance between trying new actions and using known good actions\n- **Learning Rate (α)**: How much new information overrides old information (0.1 in our implementation)\n- **Discount Factor (γ)**: Importance of future rewards (0.9 in our implementation)\n- **Epsilon (ε)**: Probability of taking random action for exploration (starts at 0.1, decays to 0.01 over time)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        self.player = 1  # This agent plays as X\n",
    "        \n",
    "    def get_action(self, state, valid_actions):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploration: choose random action\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            # Exploitation: choose best known action\n",
    "            q_values = [self.q_table[state][action] for action in valid_actions]\n",
    "            max_q = max(q_values)\n",
    "            # If multiple actions have the same max Q-value, choose randomly among them\n",
    "            best_actions = [action for action, q in zip(valid_actions, q_values) if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state, next_valid_actions):\n",
    "        # Q-Learning update rule\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if next_valid_actions:\n",
    "            # Game continues, consider future rewards\n",
    "            max_next_q = max([self.q_table[next_state][next_action] for next_action in next_valid_actions])\n",
    "        else:\n",
    "            # Game over, no future rewards\n",
    "            max_next_q = 0\n",
    "        \n",
    "        # Q(s,a) = Q(s,a) + α[r + γ*max(Q(s',a')) - Q(s,a)]\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        self.q_table[state][action] = new_q\n",
    "    \n",
    "    def decay_epsilon(self, decay_rate=0.995):\n",
    "        self.epsilon = max(0.01, self.epsilon * decay_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Opponent\n",
    "\n",
    "Let's create a simple random opponent for our agent to play against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.player = -1  # This agent plays as O\n",
    "    \n",
    "    def get_action(self, state, valid_actions):\n",
    "        return random.choice(valid_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Training the Agent\n\nNow let's train our Q-learning agent by having it play many games against the random opponent.\n\n### Training Process Explanation\n\n**What happens during training:**\n\n1. **Random Starting Player**: Each episode randomly chooses who goes first\n   - ~50% of games: Q-agent starts first (learns opening strategies)\n   - ~50% of games: Q-agent starts second (learns defensive responses)\n\n2. **Game Play**: Agent and opponent alternate moves until game ends\n\n3. **Experience Collection**: Store all moves made by Q-agent: (state, action, player)\n\n4. **Reward Assignment**: At game end, assign rewards:\n   - Win: +1 reward\n   - Loss: -1 reward\n   - Draw: 0 reward\n\n5. **Q-Table Updates**: Update Q-values for all Q-agent moves using Q-learning rule\n\n6. **Epsilon Decay**: Reduce exploration probability (ε = ε × 0.995)\n\n**Learning Evolution:**\n- **Early Training**: High exploration (ε ≈ 0.1), discovering strategies\n- **Mid Training**: Balanced exploration/exploitation, learning tactics\n- **Late Training**: Low exploration (ε ≈ 0.01), refining optimal play\n\n**What the Agent Learns:**\n- **Strategic Patterns**: Center and corner moves are strong\n- **Tactical Knowledge**: Block opponent's winning moves, take own winning moves\n- **Positional Understanding**: Creating forks and multiple threats\n- **Defensive Skills**: Counter-strategies when playing second",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(episodes=10000):\n",
    "    env = TicTacToe()\n",
    "    q_agent = QLearningAgent()\n",
    "    random_agent = RandomAgent()\n",
    "    \n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    win_history = []\n",
    "    first_player_games = 0  # Track when Q-agent goes first\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset(random_start=True)  # Randomly choose starting player\n",
    "        game_history = []  # Store (state, action, reward) for batch updates\n",
    "        \n",
    "        # Track if Q-agent starts first\n",
    "        if env.current_player == q_agent.player:\n",
    "            first_player_games += 1\n",
    "        \n",
    "        while not env.is_game_over():\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            \n",
    "            if env.current_player == q_agent.player:  # Q-learning agent's turn\n",
    "                action = q_agent.get_action(state, valid_actions)\n",
    "                env.make_move(action)\n",
    "                \n",
    "                # Store state-action pair for later update\n",
    "                game_history.append((state, action, env.current_player))\n",
    "                \n",
    "            else:  # Random agent's turn\n",
    "                action = random_agent.get_action(state, valid_actions)\n",
    "                env.make_move(action)\n",
    "            \n",
    "            env.current_player *= -1  # Switch players\n",
    "            state = env.get_state()\n",
    "        \n",
    "        # Game over, calculate rewards and update Q-table\n",
    "        final_reward = env.get_reward(q_agent.player)\n",
    "        \n",
    "        # Update Q-values for all moves made by the Q-learning agent\n",
    "        for i, (hist_state, hist_action, player) in enumerate(game_history):\n",
    "            if player == q_agent.player:\n",
    "                if i < len(game_history) - 1:\n",
    "                    next_state = game_history[i + 1][0] if i + 1 < len(game_history) else state\n",
    "                    next_valid_actions = env.get_valid_actions() if not env.is_game_over() else []\n",
    "                else:\n",
    "                    next_state = state\n",
    "                    next_valid_actions = []\n",
    "                \n",
    "                q_agent.update_q_table(hist_state, hist_action, final_reward, next_state, next_valid_actions)\n",
    "        \n",
    "        # Track results\n",
    "        if final_reward == 1:\n",
    "            wins += 1\n",
    "        elif final_reward == -1:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "        \n",
    "        # Decay epsilon for less exploration over time\n",
    "        q_agent.decay_epsilon()\n",
    "        \n",
    "        # Track win rate every 1000 episodes\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            win_rate = wins / (episode + 1)\n",
    "            first_player_rate = first_player_games / (episode + 1)\n",
    "            win_history.append(win_rate)\n",
    "            print(f\"Episode {episode + 1}: Win rate = {win_rate:.3f}, First player rate = {first_player_rate:.3f}, Epsilon = {q_agent.epsilon:.3f}\")\n",
    "    \n",
    "    print(f\"\\nFinal Results after {episodes} episodes:\")\n",
    "    print(f\"Wins: {wins} ({wins/episodes:.1%})\")\n",
    "    print(f\"Losses: {losses} ({losses/episodes:.1%})\")\n",
    "    print(f\"Draws: {draws} ({draws/episodes:.1%})\")\n",
    "    print(f\"Q-agent went first: {first_player_games} times ({first_player_games/episodes:.1%})\")\n",
    "    \n",
    "    return q_agent, win_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000: Win rate = 0.582, First player rate = 0.487, Epsilon = 0.010\n",
      "Episode 2000: Win rate = 0.653, First player rate = 0.486, Epsilon = 0.010\n",
      "Episode 3000: Win rate = 0.687, First player rate = 0.488, Epsilon = 0.010\n",
      "Episode 4000: Win rate = 0.707, First player rate = 0.483, Epsilon = 0.010\n",
      "Episode 5000: Win rate = 0.720, First player rate = 0.490, Epsilon = 0.010\n",
      "Episode 6000: Win rate = 0.734, First player rate = 0.492, Epsilon = 0.010\n",
      "Episode 7000: Win rate = 0.747, First player rate = 0.493, Epsilon = 0.010\n",
      "Episode 8000: Win rate = 0.757, First player rate = 0.493, Epsilon = 0.010\n",
      "Episode 9000: Win rate = 0.768, First player rate = 0.495, Epsilon = 0.010\n",
      "Episode 10000: Win rate = 0.773, First player rate = 0.495, Epsilon = 0.010\n",
      "Episode 11000: Win rate = 0.779, First player rate = 0.495, Epsilon = 0.010\n",
      "Episode 12000: Win rate = 0.782, First player rate = 0.496, Epsilon = 0.010\n",
      "Episode 13000: Win rate = 0.787, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 14000: Win rate = 0.792, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 15000: Win rate = 0.795, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 16000: Win rate = 0.799, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 17000: Win rate = 0.803, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 18000: Win rate = 0.805, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 19000: Win rate = 0.808, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 20000: Win rate = 0.810, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 21000: Win rate = 0.811, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 22000: Win rate = 0.813, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 23000: Win rate = 0.815, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 24000: Win rate = 0.816, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 25000: Win rate = 0.817, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 26000: Win rate = 0.818, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 27000: Win rate = 0.818, First player rate = 0.496, Epsilon = 0.010\n",
      "Episode 28000: Win rate = 0.820, First player rate = 0.496, Epsilon = 0.010\n",
      "Episode 29000: Win rate = 0.821, First player rate = 0.496, Epsilon = 0.010\n",
      "Episode 30000: Win rate = 0.822, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 31000: Win rate = 0.823, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 32000: Win rate = 0.823, First player rate = 0.497, Epsilon = 0.010\n",
      "Episode 33000: Win rate = 0.824, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 34000: Win rate = 0.825, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 35000: Win rate = 0.827, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 36000: Win rate = 0.827, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 37000: Win rate = 0.828, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 38000: Win rate = 0.828, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 39000: Win rate = 0.830, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 40000: Win rate = 0.831, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 41000: Win rate = 0.832, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 42000: Win rate = 0.832, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 43000: Win rate = 0.833, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 44000: Win rate = 0.834, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 45000: Win rate = 0.834, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 46000: Win rate = 0.834, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 47000: Win rate = 0.834, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 48000: Win rate = 0.834, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 49000: Win rate = 0.835, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 50000: Win rate = 0.835, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 51000: Win rate = 0.836, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 52000: Win rate = 0.836, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 53000: Win rate = 0.837, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 54000: Win rate = 0.837, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 55000: Win rate = 0.837, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 56000: Win rate = 0.837, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 57000: Win rate = 0.838, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 58000: Win rate = 0.838, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 59000: Win rate = 0.838, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 60000: Win rate = 0.839, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 61000: Win rate = 0.840, First player rate = 0.498, Epsilon = 0.010\n",
      "Episode 62000: Win rate = 0.840, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 63000: Win rate = 0.841, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 64000: Win rate = 0.841, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 65000: Win rate = 0.842, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 66000: Win rate = 0.842, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 67000: Win rate = 0.842, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 68000: Win rate = 0.843, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 69000: Win rate = 0.843, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 70000: Win rate = 0.843, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 71000: Win rate = 0.844, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 72000: Win rate = 0.844, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 73000: Win rate = 0.845, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 74000: Win rate = 0.845, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 75000: Win rate = 0.845, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 76000: Win rate = 0.845, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 77000: Win rate = 0.845, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 78000: Win rate = 0.846, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 79000: Win rate = 0.846, First player rate = 0.500, Epsilon = 0.010\n",
      "Episode 80000: Win rate = 0.846, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 81000: Win rate = 0.847, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 82000: Win rate = 0.847, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 83000: Win rate = 0.847, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 84000: Win rate = 0.847, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 85000: Win rate = 0.847, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 86000: Win rate = 0.848, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 87000: Win rate = 0.848, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 88000: Win rate = 0.848, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 89000: Win rate = 0.848, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 90000: Win rate = 0.848, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 91000: Win rate = 0.849, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 92000: Win rate = 0.849, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 93000: Win rate = 0.849, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 94000: Win rate = 0.850, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 95000: Win rate = 0.850, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 96000: Win rate = 0.850, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 97000: Win rate = 0.850, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 98000: Win rate = 0.850, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 99000: Win rate = 0.850, First player rate = 0.499, Epsilon = 0.010\n",
      "Episode 100000: Win rate = 0.851, First player rate = 0.499, Epsilon = 0.010\n",
      "\n",
      "Final Results after 100000 episodes:\n",
      "Wins: 85058 (85.1%)\n",
      "Losses: 5093 (5.1%)\n",
      "Draws: 9849 (9.8%)\n",
      "Q-agent went first: 49862 times (49.9%)\n"
     ]
    }
   ],
   "source": [
    "# Train the agent with random starting player\n",
    "trained_agent, win_history = train_agent(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Learning Progress\n",
    "\n",
    "Let's plot how the agent's performance improved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final win rate: 85.1%\n"
     ]
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "episodes = range(1000, len(win_history) * 1000 + 1, 1000)\n",
    "plt.plot(episodes, win_history, 'b-', linewidth=2)\n",
    "plt.title('Q-Learning Agent Performance Over Time')\n",
    "plt.xlabel('Training Episodes')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final win rate: {win_history[-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Trained Agent\n",
    "\n",
    "Let's test our trained agent by having it play a few games and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, num_games=100):\n",
    "    env = TicTacToe()\n",
    "    random_opponent = RandomAgent()\n",
    "    \n",
    "    # Set epsilon to 0 for testing (no exploration, only exploitation)\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "    \n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for game in range(num_games):\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not env.is_game_over():\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            \n",
    "            if env.current_player == 1:  # Agent's turn\n",
    "                action = agent.get_action(state, valid_actions)\n",
    "            else:  # Random opponent's turn\n",
    "                action = random_opponent.get_action(state, valid_actions)\n",
    "            \n",
    "            env.make_move(action)\n",
    "            env.current_player *= -1\n",
    "            state = env.get_state()\n",
    "        \n",
    "        result = env.get_reward(agent.player)\n",
    "        if result == 1:\n",
    "            wins += 1\n",
    "        elif result == -1:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    # Restore original epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    print(f\"Test Results over {num_games} games:\")\n",
    "    print(f\"Wins: {wins} ({wins/num_games:.1%})\")\n",
    "    print(f\"Losses: {losses} ({losses/num_games:.1%})\")\n",
    "    print(f\"Draws: {draws} ({draws/num_games:.1%})\")\n",
    "    \n",
    "    return wins, losses, draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results over 1000 games:\n",
      "Wins: 941 (94.1%)\n",
      "Losses: 0 (0.0%)\n",
      "Draws: 59 (5.9%)\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "test_results = test_agent(trained_agent, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Play Against the Agent\n\nLet's create a function to play against our trained agent interactively.\n\n### Agent Playing Behavior\n\n**Important Note**: When playing against you, the agent uses a **pure greedy policy**:\n- **Epsilon set to 0**: No random exploration during gameplay\n- **Optimal Play**: Always chooses the action with highest Q-value\n- **Maximum Challenge**: Agent plays at its full learned capability\n\nThis differs from training where the agent used epsilon-greedy (some randomness) to learn new strategies.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_against_agent(agent):\n",
    "    env = TicTacToe()\n",
    "    \n",
    "    # Set epsilon to 0 for playing (no exploration)\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "    \n",
    "    print(\"🎮 Welcome to Tic-Tac-Toe!\")\n",
    "    print(\"You are O (circles), the agent is X (crosses)\")\n",
    "    print(\"\\nBoard positions:\")\n",
    "    print(\"  1 | 2 | 3\")\n",
    "    print(\"  ---------\")\n",
    "    print(\"  4 | 5 | 6\") \n",
    "    print(\"  ---------\")\n",
    "    print(\"  7 | 8 | 9\")\n",
    "    print(\"\\nJust enter a number 1-9 to make your move!\\n\")\n",
    "    \n",
    "    # Position mapping: number -> (row, col)\n",
    "    pos_map = {\n",
    "        1: (0,0), 2: (0,1), 3: (0,2),\n",
    "        4: (1,0), 5: (1,1), 6: (1,2),\n",
    "        7: (2,0), 8: (2,1), 9: (2,2)\n",
    "    }\n",
    "    \n",
    "    # Reverse mapping: (row, col) -> number\n",
    "    reverse_map = {v: k for k, v in pos_map.items()}\n",
    "    \n",
    "    state = env.reset()\n",
    "    env.display()\n",
    "    \n",
    "    while not env.is_game_over():\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        \n",
    "        if env.current_player == 1:  # Agent's turn (X)\n",
    "            print(\"🤖 Agent's turn...\")\n",
    "            action = agent.get_action(state, valid_actions)\n",
    "            env.make_move(action)\n",
    "            agent_pos = reverse_map[action]\n",
    "            print(f\"Agent chose position {agent_pos}\")\n",
    "            \n",
    "        else:  # Human player's turn (O)\n",
    "            print(\"👤 Your turn!\")\n",
    "            valid_numbers = [reverse_map[action] for action in valid_actions]\n",
    "            print(f\"Available positions: {sorted(valid_numbers)}\")\n",
    "            \n",
    "            try:\n",
    "                move_input = input(\"Enter position (1-9): \")\n",
    "                position = int(move_input)\n",
    "                \n",
    "                if position in valid_numbers:\n",
    "                    action = pos_map[position]\n",
    "                    env.make_move(action)\n",
    "                else:\n",
    "                    print(\"❌ Invalid move! Position already taken or out of range.\")\n",
    "                    continue\n",
    "                    \n",
    "            except ValueError:\n",
    "                print(\"❌ Invalid input! Please enter a number 1-9.\")\n",
    "                continue\n",
    "        \n",
    "        env.current_player *= -1\n",
    "        state = env.get_state()\n",
    "        env.display()\n",
    "    \n",
    "    # Game over, announce result\n",
    "    winner = env.check_winner()\n",
    "    if winner == 1:\n",
    "        print(\"🤖 Agent wins!\")\n",
    "    elif winner == -1:\n",
    "        print(\"🎉 You win!\")\n",
    "    else:\n",
    "        print(\"🤝 It's a draw!\")\n",
    "    \n",
    "    # Restore original epsilon\n",
    "    agent.epsilon = original_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 Welcome to Tic-Tac-Toe!\n",
      "You are O (circles), the agent is X (crosses)\n",
      "\n",
      "Board positions:\n",
      "  1 | 2 | 3\n",
      "  ---------\n",
      "  4 | 5 | 6\n",
      "  ---------\n",
      "  7 | 8 | 9\n",
      "\n",
      "Just enter a number 1-9 to make your move!\n",
      "\n",
      "\n",
      "   0   1   2\n",
      "0    |   |  \n",
      "  -----------\n",
      "1    |   |  \n",
      "  -----------\n",
      "2    |   |  \n",
      "\n",
      "🤖 Agent's turn...\n",
      "Agent chose position 5\n",
      "\n",
      "   0   1   2\n",
      "0    |   |  \n",
      "  -----------\n",
      "1    | X |  \n",
      "  -----------\n",
      "2    |   |  \n",
      "\n",
      "👤 Your turn!\n",
      "Available positions: [1, 2, 3, 4, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Play against the trained agent!\n",
    "play_against_agent(trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "This notebook demonstrated several important reinforcement learning concepts:\n",
    "\n",
    "1. **Environment Design**: We created a tic-tac-toe environment that provides states, actions, and rewards.\n",
    "\n",
    "2. **Q-Learning Algorithm**: Our agent learned by updating Q-values based on the reward received and the maximum future reward possible.\n",
    "\n",
    "3. **Exploration vs Exploitation**: The epsilon-greedy strategy balanced trying new actions (exploration) with using known good actions (exploitation).\n",
    "\n",
    "4. **Learning Over Time**: The agent's performance improved as it played more games and learned better strategies.\n",
    "\n",
    "5. **Hyperparameter Tuning**: Learning rate, discount factor, and epsilon decay all affect the agent's learning performance.\n",
    "\n",
    "## Extensions to Try\n",
    "\n",
    "- Implement different RL algorithms (SARSA, Deep Q-Networks)\n",
    "- Train against different types of opponents (minimax, human players)\n",
    "- Experiment with different reward structures\n",
    "- Add more sophisticated state representations\n",
    "- Implement self-play training\n",
    "\n",
    "This example provides a solid foundation for understanding how reinforcement learning works in practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}